## 22/5/3

想了想，尽管是小项目，一个简短的调参笔记也是需要的。之前很多调参过程都忘记记录了。

最近的一次是加入了 batchnorm，收敛速度很明显地加快了。但是我对 batchnorm 的用法还不是很熟悉，需要多阅读和尝试。

参考了别人的结果，将 dropout 和 batchnorm 进行了对比，得出结论：

> 无论是理论上的分析，还是现代深度模型的演变，或者是实验的结果，BN技术已显示出其优于Dropout的正则化效果。

> Dropout是过去几年非常流行的正则化技术，可有效防止过拟合的发生。但从深度学习的发展趋势看，Batch Normalizaton(简称BN)正在逐步取代Dropout技术，特别是在卷积层。本文将首先引入Dropout的原理和实现，然后观察现代深度模型Dropout的使用情况，并与BN进行实验比对，从原理和实测上来说明Dropout已是过去式，大家应尽可能使用BN技术。

